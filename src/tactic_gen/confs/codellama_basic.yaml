# File Paths
data_path: "/home/ubuntu/coq-modeling/data/auto-tpe-1000"
model_name: "codellama/CodeLlama-7b-hf"
output_dir: "/home/ubuntu/coq-modeling/models/codellama-7b-tpe-1k" 

# Training Args
max_seq_len: 512
max_input_len: 448 
per_device_train_batch_size: 4
learning_rate: 1.0e-4 
num_train_epochs: 1
peft_lora_r: 64
peft_lora_alpha: 16
deepspeed: /home/ubuntu/coq-modeling/deepspeed.json

# Evaluation Args
eval_steps: 500 
eval_accumulation_steps: 1
per_device_eval_batch_size: 2
num_eval_examples: 2000 # Evaluation would take ~2 hours each time w/o limiting this

# Logging Args
logging_steps: 10
save_steps: 100 
save_total_limit: 5

# Train from checkpoint
checkpoint_name: "/home/ubuntu/coq-modeling/models/codellama-7b-tpe-1k/checkpoint-15700"

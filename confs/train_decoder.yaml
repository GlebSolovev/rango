# File Paths
data_path: data/general-random
#model_name: codellama/CodeLlama-7b-hf
model_name: deepseek-ai/deepseek-coder-1.3b-instruct
#model_name: openai-community/gpt2 
output_dir: models/deepseek-basic-rnd

# Training Args
hard_seq_len: 512
per_device_train_batch_size: 4
learning_rate: 1.0e-3
num_train_epochs: 2
peft_lora_r: 64
peft_lora_alpha: 16

# Data Format 
example_collator:
  alias: basic
  state_tokens: 140 
  script_tokens: 100 
  out_tokens: 16 

# Evaluation Args
eval_steps: 500 
save_steps: 500
eval_accumulation_steps: 1
per_device_eval_batch_size: 2
num_eval_examples: 2000 # Evaluation would take ~2 hours each time w/o limiting this

# Logging Args
logging_steps: 10
save_total_limit: 2

# Train from checkpoint
#checkpoint_name: "models/codellama-7b-whole-proof-ret/checkpoint-4500"

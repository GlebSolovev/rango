# File Paths
data_path: data/general-final
#model_name: codellama/CodeLlama-7b-hf
model_name: deepseek-ai/deepseek-coder-1.3b-instruct
#model_name: openai-community/gpt2 
output_dir: models/deepseek-proof-prem-final

# Training Args
hard_seq_len: 4096
per_device_train_batch_size: 4
learning_rate: 1.0e-3
num_train_epochs: 2
peft_lora_r: 64
peft_lora_alpha: 16

# Data Format 
# example_collator:
#   alias: basic
#   state_tokens: 500 
#   script_tokens: 300 
#   out_tokens: 32 

example_collator:
  alias: proof-premise
  state_tokens: 1024 
  script_tokens: 512
  proof_tokens: 1024 
  premise_tokens: 512 
  out_tokens: 128 


# Evaluation Args
eval_steps: 500 
save_steps: 500
eval_accumulation_steps: 1
per_device_eval_batch_size: 2
num_eval_examples: 2000 # Evaluation would take ~2 hours each time w/o limiting this

# Logging Args
logging_steps: 10
save_total_limit: 2

# Train from checkpoint
#checkpoint_name: "models/codellama-7b-whole-proof-ret/checkpoint-4500"

# File Paths
data_path: "data/whole-proof-ret"
#model_name: "google-t5/t5-small"
#model_name: "google-t5/t5-base"
model_name: Salesforce/codet5-small
output_dir: "models/codet5-fid-small-proof-ret-1024" 

# Training Args
max_encode_len: 960
max_decode_len: 64
max_num_passages: 8 # This should coincide with the number of inputs. This means if you want no passages, you need to use a base dataset with no passages.  
per_device_train_batch_size: 4
learning_rate: 1.0e-3
num_train_epochs: 50
max_steps: 130000

# Evaluation Args
eval_steps: 500
save_steps: 500
eval_accumulation_steps: 1
per_device_eval_batch_size: 2
num_eval_examples: 2000 # Evaluation would take ~2 hours each time w/o limiting this

# Logging Args
logging_steps: 100
save_total_limit: 2

# Train from checkpoint
#checkpoint_name: "/home/ubuntu/coq-modeling/models/codellama-7b-tpe-1k/checkpoint-15700"

# File Paths
data_path: "data/codebert-raw-proof-ret-random-clean"
model_name: "deepseek-ai/deepseek-coder-1.3b-instruct"
output_dir: "models/deepseek-codebert-raw-proof-random"

# Training Args
hard_seq_len: 4096
per_device_train_batch_size: 4
learning_rate: 1.0e-3
num_train_epochs: 2
max_steps: 60000
peft_lora_r: 64
peft_lora_alpha: 16

# Basic
# example_collator:
#   alias: basic
#   state_tokens: 2640
#   script_tokens: 1320
#   out_tokens: 128 

# Proofs And Premises
# example_collator:
#   alias: proof-premise
#   state_tokens: 1024 
#   script_tokens: 512
#   proof_tokens: 1024 
#   premise_tokens: 512 
#   out_tokens: 128 

# Premises only
# example_collator:
#   alias: premise
#   state_tokens: 1024 
#   script_tokens: 512
#   premise_tokens: 1536 
#   out_tokens: 128 

# Proofs only
example_collator:
  alias: proof 
  state_tokens: 1024 
  script_tokens: 512
  proof_tokens: 1536 
  out_tokens: 128 

# Prev lines 
# example_collator:
#   alias: n-prev-line 
#   state_tokens: 1024 
#   script_tokens: 512
#   prefix_tokens: 1536
#   out_tokens: 128 
#   data_loc: raw-data/coq-dataset
#   line_dict_loc: data/line-dict.json

# Evaluation Args
eval_steps: 500 
save_steps: 500
eval_accumulation_steps: 1
per_device_eval_batch_size: 2
num_eval_examples: 2000 # Evaluation would take ~2 hours each time w/o limiting this

# Logging Args
logging_steps: 100
save_total_limit: 2

# Train from checkpoint
checkpoint_name: models/deepseek-codebert-raw-proof-random/checkpoint-30500

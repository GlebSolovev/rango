{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from data_management.jsonl_utils import ExampleDB\n",
    "from tactic_gen.lm_example import LmExample\n",
    "from transformers import T5Tokenizer, AutoTokenizer, ByT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_NAME = \"train.db\"\n",
    "DATA_LOC = Path(\"/Users/kyle/Documents/research/coq-modeling/data/basic-final\") / SPLIT_NAME\n",
    "example_db = ExampleDB.load(DATA_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"google-t5/t5-small\"\n",
    "# tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# MODEL_NAME = \"google/flan-t5-small\"\n",
    "# tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MODEL_NAME = \"google/byt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# MODEL_NAME = \"amazon/chronos-t5-large\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"{\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Tokens\n",
    "The t5 tokenizer doesn't have some tokens like \"<\" in it's vocabulary. The goal of this notebook is to figure out which ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(example_db: ExampleDB, idx: int) -> LmExample:\n",
    "    obj_str = example_db.retrieve(idx + 1)\n",
    "    json_obj = json.loads(obj_str)\n",
    "    return LmExample.from_json(json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (lambda a: a + 1)\n",
    "def print_freq_dict(freq_dict: dict[str, int]):\n",
    "    sorted_items = sorted(list(freq_dict.items()), key=lambda t: -1 * t[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_freqs: dict[str, int] = {}\n",
    "#for i in range(example_db.size()):\n",
    "for i in range(100):\n",
    "    example = get_example(example_db, i)\n",
    "    out_tokens = tokenizer.tokenize(example.output)\n",
    "    out_idxs = tokenizer.encode(example.output, add_special_tokens=False)\n",
    "    assert len(out_tokens) == len(out_idxs)\n",
    "    \n",
    "    for out_tok, out_idx in zip(out_tokens, out_idxs):\n",
    "        if out_idx == tokenizer.unk_token_id:\n",
    "            if out_tok not in missing_freqs:\n",
    "                missing_freqs[out_tok] = 0\n",
    "            missing_freqs[out_tok] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_freqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/coq-modeling/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-23 00:48:35,905] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "from pathlib import Path\n",
    "from model_deployment.model_wrapper import FidT5LocalWrapper \n",
    "\n",
    "from data_management.sentence_db import SentenceDB\n",
    "from data_management.dataset_file import DatasetFile, Proof\n",
    "from data_management.splits import DATA_POINTS_NAME, REPOS_NAME, file_from_split, DataSplit, FileInfo, Split\n",
    "from data_management.create_lm_dataset import LmDatasetConf\n",
    "\n",
    "import torch\n",
    "from torch import log_softmax\n",
    "\n",
    "from tactic_gen.lm_example import LmExample, LmFormatter, formatter_from_conf\n",
    "\n",
    "from util.constants import DATA_CONF_NAME\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(Path(\"/home/ubuntu/coq-modeling\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_DB_LOC = Path(\"sentences.db\")\n",
    "DATA_LOC = Path(\"raw-data/coq-dataset\")\n",
    "DATA_SPLIT_LOC = Path(\"splits/final-split.json\")\n",
    "sentence_db = SentenceDB.load(SENTENCE_DB_LOC)\n",
    "data_split = DataSplit.load(DATA_SPLIT_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = Path(\"repos/coq-community-bertrand/theories/Binomial.v\")\n",
    "CHECKPOINT_LOC = Path(\"models/t5-fid-base-basic-final/checkpoint-110500\")\n",
    "THEOREM_NAME = \"binomial_def2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info, split = file_from_split(str(FILE_NAME), data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_wrapper = FidT5LocalWrapper.from_checkpoint(str(CHECKPOINT_LOC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatters(checkpoint_loc: Path) -> list[LmFormatter]:\n",
    "    assert 0 < len(checkpoint_loc.parents)\n",
    "    model_loc = checkpoint_loc.parents[0]\n",
    "    lm_data_conf = model_loc / DATA_CONF_NAME\n",
    "    assert lm_data_conf.exists()\n",
    "    with lm_data_conf.open(\"r\") as fin:\n",
    "        yaml_data = yaml.load(fin, Loader=yaml.Loader)\n",
    "    data_conf = LmDatasetConf.from_yaml(yaml_data)\n",
    "    formatter_confs = data_conf.lm_formatter_confs\n",
    "    formatters = [formatter_from_conf(f) for f in formatter_confs]\n",
    "    return formatters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_obj = file_info.get_dp(DATA_LOC, sentence_db)\n",
    "proof = dp_obj.get_theorem(THEOREM_NAME)\n",
    "formatter = get_formatters(CHECKPOINT_LOC)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_logits(proof: Proof, dp_obj: DatasetFile, file_info: FileInfo, split: Split, data_loc: Path, model: FidT5LocalWrapper, formatter: LmFormatter) -> tuple[list[str], list[float]]:\n",
    "    for i, step in enumerate(proof.steps[:1]):\n",
    "        example = formatter.example_from_step(\n",
    "            i,\n",
    "            proof,\n",
    "            dp_obj=dp_obj,\n",
    "            file_info=file_info,\n",
    "            split=split,\n",
    "            data_loc=data_loc,\n",
    "            ground_truth_steps=None,  # Not doing this right now\n",
    "            key_record=None,\n",
    "            cutoff_idx=None,\n",
    "        )\n",
    "        input_batch = model.local_dset.collate([example])\n",
    "        with torch.no_grad():\n",
    "            logits = model.model(\n",
    "                input_batch[\"input_ids\"].cuda(),\n",
    "                input_batch[\"attention_mask\"].cuda(),\n",
    "                input_batch[\"labels\"].cuda(),\n",
    "            ).logits\n",
    "\n",
    "        vocab_size = logits.shape[-1]\n",
    "        print(vocab_size)\n",
    "\n",
    "        # move labels to correct device to enable model parallelism\n",
    "        labels = input_batch[\"labels\"].to(logits.device)\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        logit_mat = shift_logits.view(-1, vocab_size)\n",
    "        print(logit_mat.shape)\n",
    "        print(shift_labels.shape)\n",
    "        log_probs = log_softmax(logit_mat, 1) \n",
    "        print(log_probs.shape)\n",
    "        print(labels)\n",
    "        #one_d_log_probs = log_probs[shift_labels.view(-1)]\n",
    "        #return list(shift_labels), list(one_d_log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32128\n",
      "torch.Size([63, 32128])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([63, 32128])\n",
      "tensor([[25029,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "get_tokens_and_logits(proof, dp_obj, file_info, split, DATA_LOC, model_wrapper, formatter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

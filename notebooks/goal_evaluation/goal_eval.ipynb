{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "import torch\n",
    "\n",
    "from data_management.dataset_file import Goal\n",
    "\n",
    "from goal_evaluation.goal_example import GoalExample\n",
    "from goal_evaluation.goal_data import SEP_ID, START_ID, tokenize_example\n",
    "from transformers import OPTForCausalLM, GPT2Tokenizer \n",
    "from util.constants import TRAINING_CONF_NAME \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GoalWrapper:\n",
    "    def __init__(self, goal_model: OPTForCausalLM, tokenizer: GPT2Tokenizer, max_seq_len: int):\n",
    "        self.goal_model = goal_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.sft = torch.nn.Softmax()\n",
    "    \n",
    "    def get_score_and_n_left(self, example: GoalExample) -> tuple[float, float]:\n",
    "        tokenized_input = tokenize_example(self.tokenizer, self.max_seq_len, example) \n",
    "        query = tokenized_input[\"input_ids\"] + [SEP_ID]\n",
    "        input_tensor = torch.tensor([query], device=self.goal_model.device)\n",
    "        attn_mask = torch.tensor([[1 for _ in range(len(query))]], device=self.goal_model.device)\n",
    "        model_inputs = {\n",
    "            \"input_ids\": input_tensor,\n",
    "            \"attention_mask\": attn_mask,\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            output = self.goal_model(**model_inputs)\n",
    "        n_step_logits = output.logits[0, -1, START_ID:]\n",
    "        probs = self.sft(n_step_logits)\n",
    "        exp_n_steps = probs @ torch.arange(len(probs), device=self.goal_model.device, dtype=torch.float32)\n",
    "\n",
    "        loss_inputs = input_tensor[:, :-1]\n",
    "        loss_output_logits = output.logits[:, :-1]\n",
    "        shift_logits = loss_output_logits[0, :-1, :].contiguous()\n",
    "        shift_logprobs = torch.log_softmax(shift_logits, dim=-1)\n",
    "        shift_labels = loss_inputs[0, None, 1:].contiguous()\n",
    "        scores = torch.gather(shift_logprobs, 1, shift_labels)\n",
    "        return float(exp_n_steps), float(scores.mean())\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, checkpoint_loc: Path) -> GoalWrapper:\n",
    "        model_training_conf_loc = checkpoint_loc.parents[0] / TRAINING_CONF_NAME\n",
    "        with model_training_conf_loc.open(\"r\") as fin:\n",
    "            training_conf = yaml.load(fin, Loader=yaml.Loader)\n",
    "        model = OPTForCausalLM.from_pretrained(checkpoint_loc)\n",
    "        model = model.cuda()\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(training_conf[\"model_name\"], truncation_side=\"left\")\n",
    "        print(tokenizer.truncation_side)\n",
    "        max_seq_len = training_conf[\"max_seq_len\"]\n",
    "        return cls(model, tokenizer, max_seq_len)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n"
     ]
    }
   ],
   "source": [
    "checkpoint_loc = Path(\"/home/ubuntu/coq-modeling/models/goal/checkpoint-1000\")\n",
    "w = GoalWrapper.from_checkpoint(checkpoint_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = GoalExample(\"x, y: R\\nHxy: (x <= y)%R\\n\\n(IZR (Zfloor x) <= y)%R\", 4)\n",
    "e2 = GoalExample(\"\", 1)\n",
    "\n",
    "\n",
    "e3 = GoalExample(\"A: Type\\nAe: Equiv A\\nAplus: Plus A\\nAmult: Mult A\\nAzero: Zero A\\nAone: One A\\nAnegate: Negate A\\nAdec_recip: DecRecip A\\nH: DecField A\\nH0: \\u2200 x0 y : A, Decision (x0 = y)\\nB: Type\\ne: Equiv B\\nplus0: Plus B\\nmult0: Mult B\\nzero0: Zero B\\none0: One B\\nnegate0: Negate B\\nU: IntegersToRing B\\nH1: Integers B\\nipw: Pow A B\\nIntPowSpec0: IntPowSpec A B ipw\\nx: A\\nn: B\\nEx: x = 0\\n\\n0 ^ (- n) = / 0 ^ n<G>A: Type\\nAe: Equiv A\\nAplus: Plus A\\nAmult: Mult A\\nAzero: Zero A\\nAone: One A\\nAnegate: Negate A\\nAdec_recip: DecRecip A\\nH: DecField A\\nH0: \\u2200 x0 y : A, Decision (x0 = y)\\nB: Type\\ne: Equiv B\\nplus0: Plus B\\nmult0: Mult B\\nzero0: Zero B\\none0: One B\\nnegate0: Negate B\\nU: IntegersToRing B\\nH1: Integers B\\nipw: Pow A B\\nIntPowSpec0: IntPowSpec A B ipw\\nx: A\\nn: B\\nEx: x \\u2260 0\\n\\nx ^ (- n) = / x ^ n\", 18)\n",
    "e4 = GoalExample(\"A: Type\\nAe: Equiv A\\nAplus: Plus A\\nAmult: Mult A\\nAzero: Zero A\\nAone: One A\\nAnegate: Negate A\\nAdec_recip: DecRecip A\\nH: DecField A\\nH0: \\u2200 x0 y : A, Decision (x0 = y)\\nB: Type\\ne: Equiv B\\nplus0: Plus B\\nmult0: Mult B\\nzero0: Zero B\\none0: One B\\nnegate0: Negate B\\nU: IntegersToRing B\\nH1: Integers B\\nipw: Pow A B\\nIntPowSpec0: IntPowSpec A B ipw\\nx: A\\nEx: x \\u2260 0\\n\\n\\u2200 n : B, x ^ (- n) = / x ^ n \\u2194 x ^ (- (1 + n)) = / x ^ (1 + n)\", 9)\n",
    "\n",
    "e5 = GoalExample(\"l: list nat\\n\\nl <> nil -> exists h : nat, min l = Some h\", 10)\n",
    "e6 = GoalExample(\"n: nat\\nl: list nat\\n\\nn :: l <> nil -> exists h : nat, min (n :: l) = Some h\", 10)\n",
    "\n",
    "e7 = GoalExample(\"hsfkldfh;salhfaksld;fhalsdhfsshkfla;sdfhkjal;sdfhkal;sdhkl\", 10)\n",
    "\n",
    "#e6 = GoalExample(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.69045639038086, -10.862499237060547)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e, l = w.get_score_and_n_left(e7)\n",
    "e, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSoftmax()\n\u001b[0;32m----> 2\u001b[0m log_of_interest \u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (START_ID \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):]\n\u001b[1;32m      3\u001b[0m log_of_interest[:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "sft = torch.nn.Softmax()\n",
    "log_of_interest = l.logits[0, -1, (START_ID + 1):]\n",
    "log_of_interest[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2, device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(log_of_interest) + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
